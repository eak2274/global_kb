**Compaction в Delta Lake: оптимизация хранения данных**

Compaction — это процесс объединения множества мелких файлов данных в Delta Lake в более крупные. Это позволяет оптимизировать производительность чтения, поскольку чтение меньшего количества крупных файлов обычно происходит быстрее, чем чтение множества мелких файлов.

**Сценарии использования Compaction:**

- **Частые операции записи:**
    - Если ваша таблица Delta Lake часто обновляется или в нее добавляются новые данные, это может привести к появлению большого количества мелких файлов.
    - Compaction помогает объединить эти файлы, чтобы избежать ухудшения производительности чтения.
- **Пакетная обработка:**
    - После выполнения пакетной обработки данных, которая генерирует множество мелких файлов, Compaction может быть полезен для оптимизации хранения.
- **Оптимизация запросов:**
    - Если у вас есть запросы, которые сканируют большие объемы данных, Compaction может ускорить их выполнение.

**Качества Compaction:**

- **Улучшение производительности чтения:**
    - Сокращение количества файлов, которые необходимо прочитать, приводит к ускорению запросов.
- **Снижение нагрузки на хранилище:**
    - Уменьшение количества файлов снижает нагрузку на систему хранения данных.
- **Оптимизация метаданных:**
    - Compaction также помогает оптимизировать метаданные Delta Lake, что также может улучшить производительность.

**Настройка размера и периодичности сжатия:**

- **Периодичность сжатия:**
    - Частота выполнения операции `OPTIMIZE` (которая включает сжатие) зависит от частоты изменений данных и требований к производительности.
    - Для часто обновляемых таблиц рекомендуется выполнять сжатие чаще, например, ежедневно или еженедельно.
    - Для таблиц с редкими изменениями можно выполнять сжатие реже.
    - В Databricks существует функция Auto Compaction, которая автоматически выполняет сжатие в фоновом режиме, что избавляет от необходимости ручного планирования.
- **Размер сжатых файлов:**
    - Размер сжатых файлов можно настроить, чтобы оптимизировать производительность чтения.
    - Databricks автоматически настраивает целевой размер файла, стремясь к правильным файлам размера.
    - Так же в Databricks Runtime, есть возможность настроить размер файлов записываемых в Delta Lake таблицы.
    - Рекомендации от Databricks по размеру файлов:
        - Для таблиц размером менее 1 ТБ рекомендуется использовать файлы размером 256 МБ.
        - Для таблиц размером более 1 ТБ рекомендуется использовать файлы размером 1 ГБ.
- **Конфигурация:**
    - Параметры конфигурации Spark можно использовать для настройки размера сжатых файлов.
    - Так же настройки можно выставлять на уровне таблиц Delta Lake.

**Примеры кода:**

- **SQL:**

SQL

```
OPTIMIZE deltaTableName
```

- **PySpark:**

Python

```
from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "deltaTableName")
deltaTable.optimize().executeCompaction()
```

- **Настройка размера файла через Spark Conf:**

Python

```
spark.conf.set("spark.sql.files.maxPartitionBytes", "268435456") # 256 MB
spark.conf.set("spark.sql.files.maxPartitionBytes", "1073741824") # 1GB
```

- **Настройка размера файла на уровне Delta Lake таблицы:**

Python

```
from delta.tables import DeltaTable

DeltaTable.forPath(spark, "deltaTableName").property("delta.targetFileSize", "268435456") # 256 MB
DeltaTable.forPath(spark, "deltaTableName").property("delta.targetFileSize", "1073741824") # 1GB
```

- **Включение Auto Compaction в Databricks:**
    - Auto Compaction включено по умолчанию в Databricks Runtime.
- Для выключения Auto Compaction для конкретной таблицы:

Python

```
from delta.tables import DeltaTable

DeltaTable.forPath(spark, "deltaTableName").property("delta.autoOptimize.autoCompact", "false")
```

- Для включения Auto Compaction для конкретной таблицы:

Python

```
from delta.tables import DeltaTable

DeltaTable.forPath(spark, "deltaTableName").property("delta.autoOptimize.autoCompact", "true")
```

**Дополнительные сведения:**

- Compaction — это операция, требующая ресурсов, поэтому ее следует выполнять периодически, а не после каждой записи данных.
- Databricks имеет функцию Auto Compaction, которая автоматически выполняет сжатие в фоновом режиме.
- Так же в Databricks Runtime, есть возможность настроить размер файлов записываемых в Delta Lake таблицы.
- Размер и частоту сжатия файлов можно настроить с помощью параметров конфигурации Spark, и так же на уровне Delta Lake таблиц.

Compaction — это важный инструмент для поддержания оптимальной производительности Delta Lake, особенно в сценариях с частыми операциями записи или большими объемами данных.