Давайте разберем каждую тему и подтему подробно, добавляя теоретическое объяснение, примеры кода и практические советы.

---

### **4.1.2. Adaptive Query Execution (AQE)**

**AQE** — это фреймворк для динамической оптимизации запросов, который был введен в Spark 3.0. AQE позволяет Spark адаптироваться к фактическим данным и условиям выполнения, что значительно улучшает производительность сложных запросов. Включает четыре ключевые техники:

---

#### **4.1.2.1. Dynamic Partition Pruning**

##### **Теория**
- **Партиционирование данных:** Партиции позволяют организовать данные на диске по значениям определенного столбца (например, `year`, `region`). Это ускоряет чтение данных за счет минимизации сканирования.
- **Dynamic Partition Pruning (DPP):** AQE автоматически ограничивает чтение только тех партиций, которые необходимы для выполнения запроса. Это особенно полезно при использовании фильтров в запросах.

##### **Пример использования**
```python
from pyspark.sql import SparkSession

# Создание SparkSession
spark = SparkSession.builder.appName("DynamicPartitionPruning").enableHiveSupport().getOrCreate()

# Включение AQE
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.optimizer.dynamicPartitionPruning", "true")

# Исходные данные
sales_data = [
    (1, "North", 100),
    (2, "South", 200),
    (3, "East", 150)
]
sales_columns = ["id", "region", "sales"]
sales_df = spark.createDataFrame(sales_data, sales_columns)

# Запись данных с партиционированием по полю "region"
sales_df.write.partitionBy("region").mode("overwrite").parquet("/path/to/sales")

# Чтение данных с фильтром
regions_to_filter = ["North"]
filtered_regions = spark.createDataFrame([(r,) for r in regions_to_filter], ["region"])

# Запрос с DPP
result = sales_df.join(filtered_regions, "region").select("id", "region", "sales")
result.show()
```

**Результат:**
```
+---+------+-----+
| id|region|sales|
+---+------+-----+
|  1| North|  100|
+---+------+-----+
```

Здесь:
- Spark читает только партицию `region=North`, игнорируя остальные.

##### **Практика**
- Используйте фильтры в запросах, чтобы AQE мог автоматически применить DPP.
- Проверьте, какие партиции были прочитаны, используя метод `explain()`:
  ```python
  result.explain()
  ```

---

#### **4.1.2.2. Join Strategies Optimization**

##### **Теория**
- **Стратегии join'ов:** Spark поддерживает несколько стратегий join'ов:
  - **Broadcast join:** Используется для маленьких датасетов (один из датасетов помещается в памяти).
  - **Shuffle join:** Требует перемещения данных между узлами.
  - **Sort-merge join:** Эффективен для больших датасетов.
- **Динамический выбор стратегии:** AQE анализирует размер данных на этапе выполнения и выбирает наиболее эффективную стратегию.

##### **Пример использования**
```python
# Включение AQE
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10MB")

# Датасеты
small_df = spark.createDataFrame([(1, "Alice"), (2, "Bob")], ["id", "name"])
large_df = spark.range(1, 1000000).withColumnRenamed("id", "key")

# Запрос с join
result = small_df.join(large_df, small_df["id"] == large_df["key"])
result.show()
```

Здесь:
- AQE может заменить shuffle join на broadcast join, если `small_df` достаточно маленький.

##### **Практика**
- Настройте параметр `spark.sql.autoBroadcastJoinThreshold` для управления порогом broadcast join.
- Используйте `explain()` для анализа выбранной стратегии:
  ```python
  result.explain()
  ```

---

#### **4.1.2.3. Coalescing Post-Shuffle Partitions**

##### **Теория**
- **Проблема маленьких файлов:** После операции shuffle могут возникнуть множество маленьких файлов, что увеличивает накладные расходы на чтение и запись.
- **Coalescing:** AQE автоматически объединяет маленькие файлы в более крупные после shuffle, чтобы оптимизировать параллелизм и уменьшить накладные расходы.

##### **Пример использования**
```python
# Включение AQE
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")

# Операция shuffle
df = spark.range(1, 1000000).repartition(1000)
result = df.groupBy("id").count()

# Анализ результатов
result.show()
```

Здесь:
- AQE объединяет маленькие партиции после shuffle, чтобы сбалансировать нагрузку.

##### **Практика**
- Настройте параметр `spark.sql.adaptive.coalescePartitions.initialPartitionNum` для управления начальным количеством партиций.
- Используйте Spark UI для анализа распределения партиций.

---

#### **4.1.2.4. Dynamic Skewness Optimization**

##### **Теория**
- **Перекос данных (skew):** Когда одни задачи обрабатывают значительно больше данных, чем другие, это приводит к несбалансированной нагрузке.
- **Обнаружение перекоса:** AQE анализирует распределение данных после shuffle и выявляет перекос.
- **Оптимизация:** AQE разбивает большие партиции на более мелкие, чтобы равномерно распределить нагрузку.

##### **Пример использования**
```python
# Включение AQE
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")

# Датасет с перекосом
skewed_df = spark.range(1, 1000000).withColumn("key", (spark_partition_id() % 10))
result = skewed_df.groupBy("key").count()

# Анализ результатов
result.show()
```

Здесь:
- AQE обнаруживает перекос в данных и разбивает большие партиции для балансировки нагрузки.

##### **Практика**
- Используйте Spark UI для анализа времени выполнения задач и выявления перекоса.
- Настройте параметр `spark.sql.adaptive.skewJoin.skewedPartitionFactor` для управления чувствительностью к перекосу.

---

### **Заключение**

В этом разделе мы рассмотрели четыре ключевые техники AQE:
1. **Dynamic Partition Pruning:** Минимизация сканирования данных за счет чтения только нужных партиций.
2. **Join Strategies Optimization:** Динамический выбор оптимальной стратегии join'а.
3. **Coalescing Post-Shuffle Partitions:** Объединение маленьких файлов для оптимизации параллелизма.
4. **Dynamic Skewness Optimization:** Решение проблемы перекоса данных.

Каждая из этих техник предоставляет мощные инструменты для повышения производительности Spark-приложений. Примеры кода и практические советы помогут вам внедрить эти оптимизации в своих проектах.