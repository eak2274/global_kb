
# üìö –í–≤–µ–¥–µ–Ω–∏–µ –≤ RDD –≤ Apache Spark (–Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö PySpark)

> üí° **RDD (Resilient Distributed Dataset)** ‚Äî —ç—Ç–æ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä–∞—è –¥–∞—ë—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≥–∏–±–∫–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–∞–Ω–Ω—ã–º–∏ –≤ Spark.  
> –•–æ—Ç—è DataFrame –∏ Dataset API —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç—Å—è –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á, –ø–æ–Ω–∏–º–∞–Ω–∏–µ RDD –ø–æ–º–æ–≥–∞–µ—Ç –ª—É—á—à–µ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Spark –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –±–æ–ª–µ–µ —Å—Ç–∞—Ä—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ –∫–æ–¥–∞.

---

## üîπ –ß—Ç–æ —Ç–∞–∫–æ–µ RDD?

**RDD (Resilient Distributed Dataset)** ‚Äî —ç—Ç–æ:
- –ù–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –Ω–µ–∏–∑–º–µ–Ω—è–µ–º—ã–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –æ–±—ä–µ–∫—Ç–æ–≤
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
- –ù–µ –∏–º–µ–µ—Ç —Å—Ö–µ–º—ã (schema)
- –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å JVM —á–µ—Ä–µ–∑ Py4J

### ‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:

| –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å | –û–ø–∏—Å–∞–Ω–∏–µ |
|-------------|----------|
| **–†–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å** | –î–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø–æ —É–∑–ª–∞–º –∫–ª–∞—Å—Ç–µ—Ä–∞ |
| **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å (Resilient)** | –°–ø–æ—Å–æ–±–µ–Ω –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è –ø–æ—Å–ª–µ —Å–±–æ–µ–≤ –∑–∞ —Å—á—ë—Ç –ª–æ–≥–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π |
| **–ù–µ—Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π** | –•—Ä–∞–Ω–∏—Ç –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏, —á–∏—Å–ª–∞, –∫–æ—Ä—Ç–µ–∂–∏ –∏ –¥—Ä—É–≥–∏–µ —Ç–∏–ø—ã Python –±–µ–∑ —Å—Ç—Ä–æ–≥–æ–π —Å—Ö–µ–º—ã |

---

## üîπ –°–æ–∑–¥–∞–Ω–∏–µ RDD

### 1. **–ò–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (parallelize)**

```python
rdd = sc.parallelize([1, 2, 3, 4, 5])
```

### 2. **–ß—Ç–µ–Ω–∏–µ –∏–∑ —Ñ–∞–π–ª–∞ (textFile)**

```python
rdd = sc.textFile("data.txt")
```

–ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —ç–ª–µ–º–µ–Ω—Ç–æ–º RDD.

### 3. **–ß–µ—Ä–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥—Ä—É–≥–æ–≥–æ RDD**

```python
numbers_rdd = sc.parallelize(range(10))
squared_rdd = numbers_rdd.map(lambda x: x * x)
```

---

## –£—Ä–æ–≤–µ–Ω—å –ø–∞—Ä–∞–ª–ª–µ–ª–ª–∏–∑–º–∞:
‚úÖ **–î–∞, —Ç—ã –ø—Ä–∞–≤!**  
–ü–∞—Ä–∞–º–µ—Ç—Ä `spark.hadoop.mapreduce.input.fileinputformat.split.minsize` ‚Äî **–Ω–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π –Ω–∞–ø—Ä—è–º—É—é**, –∞ **–≤–ª–∏—è–µ—Ç –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ —Å–ø–ª–∏—Ç–∞ (–∫—É—Å–∫–∞ —Ñ–∞–π–ª–∞)** –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö.

---

## üìå –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —á–∏—Å–ª–∞ –ø–∞—Ä—Ç–∏—Ü–∏–π –≤ `textFile()`:

> –û—Ç —Å–∞–º–æ–≥–æ –≤—ã—Å–æ–∫–æ–≥–æ –∫ —Å–∞–º–æ–º—É –Ω–∏–∑–∫–æ–º—É –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É:

1. **`minPartitions`** ‚Äî –µ—Å–ª–∏ —Ç—ã –ø–µ—Ä–µ–¥–∞–ª –µ–≥–æ –∫–∞–∫ –≤—Ç–æ—Ä–æ–π –∞—Ä–≥—É–º–µ–Ω—Ç –≤ `sc.textFile("path", minPartitions=...)`
2. **`spark.default.parallelism`**
3. **–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ / —Ä–∞–∑–º–µ—Ä —Å–ø–ª–∏—Ç–∞ (split size)** ‚Üí –≥–¥–µ:
   - –†–∞–∑–º–µ—Ä —Å–ø–ª–∏—Ç–∞ = **max(2 MB, spark.hadoop.mapreduce.input.fileinputformat.split.minsize)**
4. **`spark.hadoop.mapreduce.input.fileinputformat.split.minsize`** ‚Äî —Å–∞–º—ã–π –Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç

---

## üîç –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç "–ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º"?

### 1. **Spark –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä —Å–ø–ª–∏—Ç–∞:**

```python
split_size = max(2MB, spark.hadoop.mapreduce.input.fileinputformat.split.minsize)
```

- –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é Spark –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **2 MB** –∫–∞–∫ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ñ–∞–π–ª–∞.
- –ù–æ —Ç—ã –º–æ–∂–µ—à—å —É–≤–µ–ª–∏—á–∏—Ç—å —ç—Ç–æ—Ç –ø–æ—Ä–æ–≥, —á—Ç–æ–±—ã **—Å–Ω–∏–∑–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π**, –Ω–∞–ø—Ä–∏–º–µ—Ä:

```bash
--conf spark.hadoop.mapreduce.input.fileinputformat.split.minsize=134217728  # 128 MB
```

‚û°Ô∏è –¢–æ–≥–¥–∞ –æ–¥–∏–Ω —Å–ø–ª–∏—Ç –±—É–¥–µ—Ç –≤–µ—Å–∏—Ç—å –º–∏–Ω–∏–º—É–º 128 MB ‚Üí –º–µ–Ω—å—à–µ –ø–∞—Ä—Ç–∏—Ü–∏–π ‚Üí –º–µ–Ω—å—à–µ –∑–∞–¥–∞—á.

---

### 2. **–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π –∏–∑ —Ñ–∞–π–ª–∞:**

```python
num_of_partitions_from_file = ceil(file_size / split_size)
```

–ü—Ä–∏–º–µ—Ä:
- –§–∞–π–ª = 500 MB
- split_size = 128 MB
- ‚Üí `500 / 128 ‚âà 3.9`, –∑–Ω–∞—á–∏—Ç: **4 –ø–∞—Ä—Ç–∏—Ü–∏–∏**

---

### 3. **–ë–µ—Ä—ë—Ç—Å—è –º–∞–∫—Å–∏–º—É–º –º–µ–∂–¥—É —ç—Ç–∏–º —á–∏—Å–ª–æ–º –∏ `defaultParallelism`:**

```python
final_num_partitions = max(num_of_partitions_from_file, defaultParallelism)
```

–ï—Å–ª–∏ `defaultParallelism = 8`, –∞ —Ñ–∞–π–ª –¥–∞—ë—Ç —Ç–æ–ª—å–∫–æ 4 –ø–∞—Ä—Ç–∏—Ü–∏–∏ ‚Üí –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **8 –ø–∞—Ä—Ç–∏—Ü–∏–π**

---

## üß† –í–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å:

| –ß—Ç–æ –∑–∞–¥–∞—ë—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å | –ß—Ç–æ –¥–µ–ª–∞–µ—Ç Spark |
|-------------------------|------------------|
| `minPartitions` –≤ `textFile(...)` | –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ **–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π** |
| `spark.default.parallelism` | –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç **–±–∞–∑–æ–≤–æ–µ —á–∏—Å–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞** |
| `spark.hadoop.mapreduce.input.fileinputformat.split.minsize` | –£–ø—Ä–∞–≤–ª—è–µ—Ç **—Ä–∞–∑–º–µ—Ä–æ–º —Å–ø–ª–∏—Ç–∞**, –≤–ª–∏—è–µ—Ç –Ω–∞ —á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π —á–µ—Ä–µ–∑ —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö |

---

## ‚úÖ –ü—Ä–∏–º–µ—Ä—ã:

### 1. **–§–∞–π–ª: 100 MB**, `spark.default.parallelism = 8`, `split.minsize = 2MB`

- –°–ø–ª–∏—Ç–æ–≤: `100 / 2 = 50` –ø–∞—Ä—Ç–∏—Ü–∏–π
- defaultParallelism: `8`
- ‚Üí –∏—Ç–æ–≥: **50 –ø–∞—Ä—Ç–∏—Ü–∏–π**

### 2. **–§–∞–π–ª: 100 MB**, `spark.default.parallelism = 100`, `split.minsize = 2MB`

- –°–ø–ª–∏—Ç–æ–≤: `50`
- defaultParallelism: `100`
- ‚Üí –∏—Ç–æ–≥: **100 –ø–∞—Ä—Ç–∏—Ü–∏–π**

### 3. **–§–∞–π–ª: 100 MB**, `spark.default.parallelism = 10`, `split.minsize = 25MB`

- –°–ø–ª–∏—Ç–æ–≤: `100 / 25 = 4` –ø–∞—Ä—Ç–∏—Ü–∏–∏
- defaultParallelism: `10`
- ‚Üí –∏—Ç–æ–≥: **10 –ø–∞—Ä—Ç–∏—Ü–∏–π**

### 4. **–§–∞–π–ª: 100 MB**, `spark.default.parallelism = 10`, `split.minsize = 25MB`, –Ω–æ —É–∫–∞–∑–∞–Ω `minPartitions=20`

- –°–ø–ª–∏—Ç–æ–≤: `4`
- defaultParallelism: `10`
- minPartitions: `20`
- ‚Üí –∏—Ç–æ–≥: **20 –ø–∞—Ä—Ç–∏—Ü–∏–π**

---

## üéØ –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤:

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç | –†–æ–ª—å |
|----------|-----------|------|
| `minPartitions` –≤ `textFile(path, minPartitions=...)` | üîπ –í—ã—Å–æ–∫–∏–π | –Ø–≤–Ω–æ –∑–∞–¥–∞—ë—Ç **–º–∏–Ω–∏–º—É–º –ø–∞—Ä—Ç–∏—Ü–∏–π**
| `spark.default.parallelism` | üî∏ –°—Ä–µ–¥–Ω–∏–π | –ó–∞–¥–∞—ë—Ç **–±–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞**
| –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ / split size | üî∫ –ë–∞–∑–æ–≤—ã–π | –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ —Ñ–æ—Ä–º—É–ª–µ `file_size / split_size`
| `spark.hadoop.mapreduce.input.fileinputformat.split.minsize` | üîª –ù–∏–∑–∫–∏–π | –í–ª–∏—è–µ—Ç –Ω–∞ **—Ä–∞–∑–º–µ—Ä —Å–ø–ª–∏—Ç–∞**, –∞ –Ω–µ –Ω–∞ —á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π –Ω–∞–ø—Ä—è–º—É—é

---

## üìù –°–æ–≤–µ—Ç:

–ï—Å–ª–∏ —Ç–µ–±–µ –Ω—É–∂–Ω–æ **—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å —á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π**, –¥–µ–ª–∞–π —Ç–∞–∫:

```python
# –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º —á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
rdd = sc.textFile("data.txt")
rdd = rdd.partitionBy(Partitioner.defaultPartitioner(rdd, numPartitions=10))
```

–ò–ª–∏ –ª—É—á—à–µ —Å—Ä–∞–∑—É —á–∏—Ç–∞–π —á–µ—Ä–µ–∑ DataFrame API —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —à–∞—Ñ–ª–∞:
```python
df = spark.read.option("header", "true").csv("data.csv")
repartitioned_df = df.repartition("some_column")
```

---

## ‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∏—Å–ª–∞ –ø–∞—Ä—Ç–∏—Ü–∏–π –≤ `sc.parallelize(...)`

Spark –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ª–µ–¥—É—é—â—É—é **–ª–æ–≥–∏–∫—É –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∏—Å–ª–∞ –ø–∞—Ä—Ç–∏—Ü–∏–π**, –µ—Å–ª–∏ —Ç—ã –Ω–µ —É–∫–∞–∑—ã–≤–∞–µ—à—å –µ–≥–æ —è–≤–Ω–æ –≤ –º–µ—Ç–æ–¥–µ `parallelize(data, numSlices=...)`:

### üîπ –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ (–æ—Ç —Å–∞–º–æ–≥–æ –≤—ã—Å–æ–∫–æ–≥–æ –∫ –Ω–∏–∑–∫–æ–º—É):

| –£—Ä–æ–≤–µ–Ω—å | –ß—Ç–æ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è | –û–ø–∏—Å–∞–Ω–∏–µ |
|--------|------------------|----------|
| 1 | **`numSlices`** | –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —è–≤–Ω–æ –≤ `parallelize(data, numSlices=...)`, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–Ω–æ. |
| 2 | **`spark.default.parallelism`** | –ï—Å–ª–∏ `numSlices` –Ω–µ –∑–∞–¥–∞–Ω, Spark –±–µ—Ä—ë—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. |
| 3 | **–†–µ–∂–∏–º –∑–∞–ø—É—Å–∫–∞ (local –∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä)** | –ï—Å–ª–∏ –Ω–∏ —Ç–æ, –Ω–∏ –¥—Ä—É–≥–æ–µ –Ω–µ –∑–∞–¥–∞–Ω–æ, Spark –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏: <br> - –í `local[*]`: –ø–æ —á–∏—Å–ª—É —è–¥–µ—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ <br> - –í –∫–ª–∞—Å—Ç–µ—Ä–µ: –ø–æ —á–∏—Å–ª—É –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —è–¥–µ—Ä / —Å–ª–æ—Ç–æ–≤ |

---

## üìå –ü—Ä–∏–º–µ—Ä:

```python
rdd = sc.parallelize(range(100), numSlices=10)  # => 10 –ø–∞—Ä—Ç–∏—Ü–∏–π

rdd = sc.parallelize(range(100))  # => –∑–∞–≤–∏—Å–∏—Ç –æ—Ç spark.default.parallelism –∏–ª–∏ —á–∏—Å–ª–∞ —è–¥–µ—Ä
```

---

## üß† –ö–∞–∫ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ `defaultParallelism`?

```python
print(sc.defaultParallelism)
```

‚û°Ô∏è –≠—Ç–æ —á–∏—Å–ª–æ –∏ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–∞–∫ **—á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é**, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ –∏–Ω–∞—á–µ.

---

## üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏:

–≠—Ç–∞ –∂–µ –ª–æ–≥–∏–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –¥–ª—è –¥—Ä—É–≥–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π, –Ω–∞–ø—Ä–∏–º–µ—Ä:
- `textFile()` ‚Äî –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –Ω–∞ `min(defaultParallelism, —á–∏—Å–ª–æ –±–ª–æ–∫–æ–≤ HDFS)`
- `makeRDD(...)` ‚Äî —Ç–∞–∫–∂–µ —É–≤–∞–∂–∞–µ—Ç `defaultParallelism`, –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞—Ç—å `numSlices`

---

## üß™ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä:

```python
from pyspark import SparkConf, SparkContext

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
conf = SparkConf().setAppName("ParallelismTest").setMaster("local[8]") \
                  .set("spark.default.parallelism", "4")
sc = SparkContext(conf=conf)

# –°–æ–∑–¥–∞–µ–º RDD –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è numSlices
rdd = sc.parallelize(range(100))
print(rdd.getNumPartitions())  # => 4 (–±–µ—Ä—ë—Ç—Å—è –∏–∑ spark.default.parallelism)

# –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º —á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π
rdd2 = sc.parallelize(range(100), numSlices=10)
print(rdd2.getNumPartitions())  # => 10 (—è–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)

# –ë–µ–∑ numSlices –∏ –±–µ–∑ spark.default.parallelism
sc.stop()

# –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –±–µ–∑ spark.default.parallelism
conf2 = SparkConf().setAppName("DefaultParallelismTest").setMaster("local[8]")
sc2 = SparkContext(conf2)
rdd3 = sc2.parallelize(range(100))
print(rdd3.getNumPartitions())  # => 8 (–ø–æ —á–∏—Å–ª—É —è–¥–µ—Ä –≤ local[8])
```

---

## üéØ –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞:

| –ß—Ç–æ —É–∫–∞–∑–∞–Ω–æ?                           | –ß–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π                                                     |
| -------------------------------------- | ------------------------------------------------------------------ |
| –¢–æ–ª—å–∫–æ `numSlices=...`                 | ‚úÖ –ë–µ—Ä—ë—Ç—Å—è `numSlices`                                              |
| –¢–æ–ª—å–∫–æ `spark.default.parallelism=...` | ‚úÖ –ë–µ—Ä—ë—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞                                               |
| –ù–∏—á–µ–≥–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –Ω–æ –µ—Å—Ç—å `local[N]`  | ‚úÖ –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é `N` –ø–∞—Ä—Ç–∏—Ü–∏–π                                        |
| –ù–∏—á–µ–≥–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º    | ‚úÖ –ó–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ Spark                             |
| –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ (`textFile`)              | ‚ùó –ú–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏ `spark.default.parallelism`, –∏ —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ |

---

## üîπ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (Transformations)

–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äî **–ª–µ–Ω–∏–≤—ã–µ** –æ–ø–µ—Ä–∞—Ü–∏–∏, —Å–æ–∑–¥–∞—é—â–∏–µ –Ω–æ–≤—ã–π RDD.

| –ú–µ—Ç–æ–¥ | –û–ø–∏—Å–∞–Ω–∏–µ |
|-------|----------|
| `map(func)` | –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –∫ –∫–∞–∂–¥–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É |
| `filter(func)` | –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—Ç —É—Å–ª–æ–≤–∏—é |
| `flatMap(func)` | –¢–æ –∂–µ, —á—Ç–æ map, –Ω–æ –º–æ–∂–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ |
| `mapPartitions(func)` | –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –∫ –ø–∞—Ä—Ç–∏—Ü–∏—è–º, –∞ –Ω–µ –∫ –æ—Ç–¥–µ–ª—å–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º |
| `union(rdd)` | –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–≤–∞ RDD |
| `distinct()` | –£–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã |
| `groupByKey()` | –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –∫–ª—é—á—É (–¥–ª—è –ø–∞—Ä–Ω—ã—Ö RDD) |
| `reduceByKey(func)` | –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –∫–ª—é—á—É (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ groupByKey) |

### ‚úÖ –ü—Ä–∏–º–µ—Ä—ã:

```python
# map
squared = rdd.map(lambda x: x * x)

# filter
evens = rdd.filter(lambda x: x % 2 == 0)

# flatMap
words = sc.parallelize(["Hello world", "Apache Spark"])
word_rdd = words.flatMap(lambda line: line.split(" "))

# reduceByKey
pairs = sc.parallelize([("a", 1), ("b", 1), ("a", 1)])
summed = pairs.reduceByKey(lambda a, b: a + b)
```

---

## üîπ –î–µ–π—Å—Ç–≤–∏—è (Actions)

–î–µ–π—Å—Ç–≤–∏—è ‚Äî **–≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ**, –∑–∞–ø—É—Å–∫–∞—é—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è.

| –ú–µ—Ç–æ–¥ | –û–ø–∏—Å–∞–Ω–∏–µ |
|-------|----------|
| `collect()` | –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –≤–∏–¥–µ —Å–ø–∏—Å–∫–∞ |
| `count()` | –°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ |
| `first()` | –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç |
| `take(n)` | –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–µ n —ç–ª–µ–º–µ–Ω—Ç–æ–≤ |
| `foreach(func)` | –í—ã–ø–æ–ª–Ω—è–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞–¥ –∫–∞–∂–¥—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª) |
| `saveAsTextFile(path)` | –°–æ—Ö—Ä–∞–Ω—è–µ—Ç RDD –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã |

### ‚úÖ –ü—Ä–∏–º–µ—Ä—ã:

```python
# count
print(rdd.count())

# collect
result = squared.collect()
print(result)

# saveAsTextFile
squared.saveAsTextFile("output/squared")
```

---

## üîπ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è RDD

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|
| `spark.default.parallelism` | –ß–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ RDD |
| `spark.rdd.compress` | –í–∫–ª—é—á–∞–µ—Ç —Å–∂–∞—Ç–∏–µ RDD –ø–µ—Ä–µ–¥ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π |
| `spark.serializer` | –ó–∞–¥–∞—ë—Ç —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ç–æ—Ä (Java vs Kryo) |
| `spark.storage.memoryFraction` | –î–æ–ª—è –ø–∞–º—è—Ç–∏, –¥–æ—Å—Ç—É–ø–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è RDD |
| `spark.shuffle.memoryFraction` | –î–æ–ª—è –ø–∞–º—è—Ç–∏, –≤—ã–¥–µ–ª–µ–Ω–Ω–∞—è –¥–ª—è —à–∞—Ñ–ª–æ–≤ |

–ü—Ä–∏–º–µ—Ä –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:

```bash
--conf spark.default.parallelism=8 \
--conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
--conf spark.rdd.compress=true
```

---

## üîπ –†–∞–±–æ—Ç–∞ —Å –∫–ª—é—á-–∑–Ω–∞—á–µ–Ω–∏–µ (Pair RDD)

RDD –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å **–∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è**, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏.

### –°–æ–∑–¥–∞–Ω–∏–µ Pair RDD:

```python
pair_rdd = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])
```

### –ú–µ—Ç–æ–¥—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–ª—é—á–∞–º–∏:

| –ú–µ—Ç–æ–¥ | –û–ø–∏—Å–∞–Ω–∏–µ |
|-------|----------|
| `reduceByKey(func)` | –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –∫–ª—é—á—É |
| `groupByKey()` | –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –≤—Å–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –∫–ª—é—á—É |
| `sortByKey()` | –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á—É |
| `join(rdd)` | –î–∂–æ–π–Ω –¥–≤—É—Ö –ø–∞—Ä–Ω—ã—Ö RDD |
| `keys()`, `values()` | –ü–æ–ª—É—á–∏—Ç—å –∫–ª—é—á–∏ –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è |

### ‚úÖ –ü—Ä–∏–º–µ—Ä—ã:

```python
# reduceByKey
summed = pair_rdd.reduceByKey(lambda a, b: a + b)

# join
other_rdd = sc.parallelize([("a", "x"), ("b", "y")])
joined = pair_rdd.join(other_rdd)  # (("a", (1, "x")), ("b", (2, "y")))
```

---

## üîπ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ

| –ú–µ—Ç–æ–¥ | –û–ø–∏—Å–∞–Ω–∏–µ |
|--------|-----------|
| `cache()` | –°–æ—Ö—Ä–∞–Ω—è–µ—Ç RDD –≤ –ø–∞–º—è—Ç–∏ |
| `persist(storage_level)` | –°–æ—Ö—Ä–∞–Ω—è–µ—Ç RDD —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —É—Ä–æ–≤–Ω—è —Ö—Ä–∞–Ω–µ–Ω–∏—è |
| `unpersist()` | –£–¥–∞–ª—è–µ—Ç –∫—ç—à |

### ‚úÖ –ü—Ä–∏–º–µ—Ä—ã:

```python
rdd.cache()

from pyspark import StorageLevel
rdd.persist(StorageLevel.MEMORY_AND_DISK)

rdd.unpersist()
```

---

## üîπ –®–∞—Ñ–ª (Shuffle)

–®–∞—Ñ–ª ‚Äî —ç—Ç–æ –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É —É–∑–ª–∞–º–∏.  
–û–ø–µ—Ä–∞—Ü–∏–∏, –≤—ã–∑—ã–≤–∞—é—â–∏–µ —à–∞—Ñ–ª:

- `groupByKey()`
- `reduceByKey()`
- `sortByKey()`
- `join()`

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|----------|
| `spark.shuffle.file.buffer` | –†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ —à–∞—Ñ–ª-—Ñ–∞–π–ª–æ–≤ –Ω–∞ –¥–∏—Å–∫ |
| `spark.reducer.maxSizeInFlight` | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –æ–±—ä—ë–º –¥–∞–Ω–Ω—ã—Ö, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã—Ö –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ |
| `spark.shuffle.compress` | –í–∫–ª—é—á–∞–µ—Ç/–æ—Ç–∫–ª—é—á–∞–µ—Ç —Å–∂–∞—Ç–∏–µ —à–∞—Ñ–ª-–¥–∞–Ω–Ω—ã—Ö |

---

## üîπ –ë—Ä–æ–¥–∫–∞—Å—Ç –∏ –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä—ã

### Broadcast (–æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è —Ä–∞—Å—Å—ã–ª–∫–∞):
```python
broadcast_var = sc.broadcast([1, 2, 3])
print(broadcast_var.value)
```

### Accumulator (–∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä—ã):
```python
acc = sc.accumulator(0)
def add_values(x):
    global acc
    acc += x

rdd.foreach(add_values)
print(acc.value)
```

---

## üîπ –û—Ç–ª–∞–¥–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑

| –ú–µ—Ç–æ–¥ | –û–ø–∏—Å–∞–Ω–∏–µ |
|--------|----------|
| `getNumPartitions()` | –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π |
| `glom().map(len).collect()` | –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä—ã –ø–∞—Ä—Ç–∏—Ü–∏–π |
| `partitions.size` | –°–º–æ—Ç—Ä–∏—Ç —á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π —É —Ç–µ–∫—É—â–µ–≥–æ RDD |

---

## üîπ –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RDD?

‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π RDD, –µ—Å–ª–∏:

- –¢–µ–±–µ –Ω—É–∂–Ω–∞ **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–∏–±–∫–æ—Å—Ç—å**
- –¢—ã —Ä–∞–±–æ—Ç–∞–µ—à—å —Å **GraphX**, **MLlib (—É—Å—Ç–∞—Ä–µ–≤—à–∏–π)** –∏–ª–∏ **DStreams**
- –¢—ã —Ö–æ—á–µ—à—å –ø–æ–Ω—è—Ç—å, **–∫–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç DataFrame –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º**
- –¢—ã –ø–∏—à–µ—à—å **–Ω–µ—Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –¥–∞–Ω–Ω—ã—Ö**

‚ùå –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π, –µ—Å–ª–∏:

- –¢—ã –ø–∏—à–µ—à—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
- –¢—ã —Ö–æ—á–µ—à—å –ø–∏—Å–∞—Ç—å —á–∏—Å—Ç—ã–π –∏ –ø—Ä–æ—Å—Ç–æ–π –∫–æ–¥
- –¢—ã —Ö–æ—á–µ—à—å –ø–æ–ª—É—á–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é

---

## üîπ –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã

```python
# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ RDD
rdd.getNumPartitions()        # —á–∏—Å–ª–æ –ø–∞—Ä—Ç–∏—Ü–∏–π
rdd.partitions               # –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞—Ä—Ç–∏—Ü–∏—è—Ö
rdd.is_cached               # –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω –ª–∏ RDD

# –ò–∑–º–µ–Ω–µ–Ω–∏–µ —á–∏—Å–ª–∞ –ø–∞—Ä—Ç–∏—Ü–∏–π
rdd.repartition(10)
rdd.coalesce(2)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞
rdd.saveAsTextFile("path/to/file")
loaded_rdd = sc.textFile("path/to/file")
```

---

## üîπ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

| –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å | DataFrame | RDD |
|-------------|-----------|-----|
| –õ—ë–≥–∫–∏–π API | ‚úÖ | ‚ùå |
| –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è | ‚úÖ | ‚ùå |
| –ü–æ–¥–¥–µ—Ä–∂–∫–∞ SQL | ‚úÖ | ‚ùå |
| –ì–∏–±–∫–æ—Å—Ç—å | ‚ùå | ‚úÖ |
| –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –Ω–æ–≤—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤ | ‚úÖ | ‚ö†Ô∏è –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç –≤—ã–±–æ—Ä–∞ |
| –ü–æ–ª–µ–∑–µ–Ω –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è Spark | ‚úÖ | ‚úÖ |

---

üìå **–í—ã–≤–æ–¥:**  
RDD ‚Äî –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –Ω–æ –æ–Ω **–Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º —Ä–∞–±–æ—Ç—ã** –≤ –Ω–æ–≤—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö. –û–¥–Ω–∞–∫–æ –µ–≥–æ –∑–Ω–∞–Ω–∏–µ **–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Spark** –∏ —á—Ç–µ–Ω–∏—è —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ –∫–æ–¥–∞.

---

–ï—Å–ª–∏ —Ç—ã —Ö–æ—á–µ—à—å, –º–æ–≥—É –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –ø–æ:

- DataFrame API
- Structured Streaming
- Catalyst Optimizer
- Adaptive Query Execution (AQE)
- MLlib –∏ GraphFrames

–ü–∏—à–∏, —á—Ç–æ —Ç–µ–±—è –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç ‚Äî –∏ —è –ø—Ä–æ–¥–æ–ª–∂—É!